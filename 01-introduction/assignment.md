# Assignment: Introduction to LangChain

## Overview

Now that you've learned the basics of LangChain, it's time to practice! These challenges will help reinforce what you've learned about models, messages, and making your first LLM calls.

## Prerequisites

- Read and studied this [lab](./README.md)
- Run all code examples in this lab

---

## Challenge: Experiment with System Prompts ğŸ­

**Goal**: Learn how system messages affect AI behavior.

**Tasks**:
1. Create a file called `personality_test.py`
2. Test the same question with three different system prompts:
   - A pirate personality
   - A professional business analyst
   - A friendly teacher for kids
3. Display all three responses side-by-side

**Example System Prompts**:
- Pirate: `"You are a pirate. Answer all questions in pirate speak with 'Arrr!' and nautical terms."`
- Analyst: `"You are a professional business analyst. Give precise, data-driven answers."`
- Teacher: `"You are a friendly teacher explaining concepts to 8-year-old children."`

**Question to Test**: "What is artificial intelligence?"

**Success Criteria**:
- Same question gets three very different response styles
- You understand how SystemMessage shapes the AI's personality

**Hints**:
```python
# 1. Import required modules
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from dotenv import load_dotenv

# 2. Load environment variables
load_dotenv()

# 3. Create the ChatOpenAI model (reuse for all personalities)

# 4. Define a list of personalities with name and system prompt

# 5. Define the question to test

# 6. Loop through each personality:
#    - Create messages list with SystemMessage and HumanMessage
#    - Invoke the model with the messages
#    - Display the response with personality name
```

---

## Bonus Challenge: Model Performance Comparison ğŸ”¬

**Goal**: Compare multiple models on the same task.

**Tasks**:
1. Create a file called `model_performance.py`
2. Test at least 2 models available on GitHub Models:
   - `gpt-5`
   - `gpt-5-mini`
3. For each model, measure:
   - Response time
   - Response length (character count)
   - Response quality (your subjective assessment)
4. Create a simple table showing the results

**Test Question**: "Explain the difference between machine learning and deep learning."

**Expected Output**:
```
ğŸ“Š Model Performance Comparison
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model          | Time    | Length | Quality
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
gpt-5-mini    | 567ms   | 234ch  | â­â­â­â­
gpt-5         | 1234ms  | 456ch  | â­â­â­â­â­
```

**Success Criteria**:
- Script compares at least 2 models
- Results are displayed in a clear format
- You can explain which model you'd choose for different use cases

**Hints**:
```python
# 1. Import required modules
import time
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# 2. Load environment variables
load_dotenv()

# 3. Define question and models list
question = "Explain the difference between machine learning and deep learning."

models = [
    {"name": "gpt-5", "description": "Most capable"},
    {"name": "gpt-5-mini", "description": "Fast and efficient"},
]

# 4. Create a function to test each model:
#    - Accept model_name as parameter
#    - Create ChatOpenAI instance with that model
#    - Measure start time with time.time()
#    - Invoke the model with the question
#    - Measure end time and calculate duration
#    - Return a dict with name, time, length, and response

# 5. Loop through models list:
#    - Call test_model() for each model
#    - Display results in a formatted table
#    - Use .ljust() for consistent column widths
```

---

## Solutions

Solutions for all challenges will be available in the [`solution/`](./solution/) folder. Try to complete the challenges on your own first before looking at the solutions!

**Additional Examples**: Check out the [`samples/`](./samples/) folder for more example solutions that demonstrate other useful concepts!

---

## Need Help?

- **Stuck on code**: Review the examples in [`code/`](./code/)
- **Error messages**: Check [Course Setup](../00-course-setup/README.md) troubleshooting
- **Concepts unclear**: Re-read and study this [lab](./README.md)
